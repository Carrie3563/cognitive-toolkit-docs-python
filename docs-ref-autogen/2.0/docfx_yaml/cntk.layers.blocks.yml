api_name: []
items:
- _type: module
  children: []
  module: cntk.layers.blocks
  name: cntk.layers.blocks
  summary: 'Basic building blocks that are semantically not layers (not used in a
    layered fashion),

    e.g. the LSTM block.

    '
  type: Namespace
  uid: cntk.layers.blocks
- _type: class
  children:
  - cntk.layers.blocks.ForwardDeclaration
  - cntk.layers.blocks.GRU
  - cntk.layers.blocks.LSTM
  - cntk.layers.blocks.RNNStep
  - cntk.layers.blocks.RNNUnit
  - cntk.layers.blocks.Stabilizer
  - cntk.layers.blocks.UntestedBranchError
  module: cntk.layers.blocks
  name: cntk.layers.blocks.Global
  summary: Proxy object to hold module level functions
  type: Class
  uid: cntk.layers.blocks.Global
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.ForwardDeclaration
  summary: "Helper for recurrent network declarations.\nReturns a placeholder variable\
    \ with an added method ``resolve_to()`` to be called\nat the end to close the\
    \ loop.\nThis is used for explicit graph building with recurrent connections.\n\
    \n.. admonition:: Example\n\n   >>> # create a graph with a recurrent loop to\
    \ compute the length of an input sequence\n   >>> from cntk.layers.typing import\
    \ *\n   >>> x = C.input_variable(**Sequence[Tensor[2]])\n   >>> ones_like_input\
    \ = C.sequence.broadcast_as(1, x)  # sequence of scalar ones of same length as\
    \ input\n   >>> out_fwd = ForwardDeclaration()  # placeholder for the state variables\n\
    \   >>> out = C.sequence.past_value(out_fwd, initial_state=0) + ones_like_input\n\
    \   >>> out_fwd.resolve_to(out)\n   >>> length = C.sequence.last(out)\n   >>>\
    \ x0 = np.reshape(np.arange(6,dtype=np.float32),(1,3,2))\n   >>> x0\n       array([[[\
    \ 0.,  1.],\n               [ 2.,  3.],\n               [ 4.,  5.]]], dtype=float32)\n\
    \   >>> length(x0)\n       array([ 3.], dtype=float32)\n\n:returns: a placeholder\
    \ variable with a method ``resolve_to()`` that resolves it to another variable\n\
    :rtype: :class:`~cntk.variables.Variable`\n"
  type: Method
  uid: cntk.layers.blocks.ForwardDeclaration
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.GRU
  summary: "Layer factory function to create a GRU block for use inside a recurrence.\n\
    The GRU block implements one step of the recurrence and is stateless. It accepts\
    \ the previous state as its first argument,\nand outputs its new state.\n\n..\
    \ admonition:: Example\n\n   >>> # a gated recurrent layer\n   >>> from cntk.layers\
    \ import *\n   >>> gru_layer = Recurrence(GRU(500))\n\n:param shape: vector or\
    \ tensor dimension of the output of this layer\n:type shape: `int` or `tuple`\
    \ of `ints`\n:param cell_shape: if given, then the output state is first computed\
    \ at `cell_shape`\n                   and linearly projected to `shape`\n:type\
    \ cell_shape: tuple, defaults to `None`\n:param activation: function to apply\
    \ at the end, e.g. `relu`\n:type activation: :class:`~cntk.ops.functions.Function`,\
    \ defaults to :func:`~cntk.ops.tanh`\n:param init: initial value of weights `W`\n\
    :type init: scalar or NumPy array or :mod:`cntk.initializer`, defaults to `glorot_uniform`\n\
    :param init_bias: initial value of weights `b`\n:type init_bias: scalar or NumPy\
    \ array or :mod:`cntk.initializer`, defaults to 0\n:param enable_self_stabilization:\
    \ if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`\n              \
    \                    to all state-related projections (but not the data input)\n\
    :type enable_self_stabilization: bool, defaults to `False`\n:param name: the name\
    \ of the Function instance in the network\n:type name: str, defaults to ''\n\n\
    :returns: A function ``(prev_h, input) -> h`` that implements one step of a recurrent\
    \ GRU layer.\n:rtype: :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.layers.blocks.GRU
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.LSTM
  summary: "Layer factory function to create an LSTM block for use inside a recurrence.\n\
    The LSTM block implements one step of the recurrence and is stateless. It accepts\
    \ the previous state as its first two arguments,\nand outputs its new state as\
    \ a two-valued tuple ``(h,c)``.\n\n.. admonition:: Example\n\n   >>> # a typical\
    \ recurrent LSTM layer\n   >>> from cntk.layers import *\n   >>> lstm_layer =\
    \ Recurrence(LSTM(500))\n\n:param shape: vector or tensor dimension of the output\
    \ of this layer\n:type shape: `int` or `tuple` of `ints`\n:param cell_shape: if\
    \ given, then the output state is first computed at `cell_shape`\n           \
    \        and linearly projected to `shape`\n:type cell_shape: tuple, defaults\
    \ to `None`\n:param activation: function to apply at the end, e.g. `relu`\n:type\
    \ activation: :class:`~cntk.ops.functions.Function`, defaults to :func:`~cntk.ops.tanh`\n\
    :param use_peepholes:\n:type use_peepholes: bool, defaults to `False`\n:param\
    \ init: initial value of weights `W`\n:type init: scalar or NumPy array or :mod:`cntk.initializer`,\
    \ defaults to `glorot_uniform`\n:param init_bias: initial value of weights `b`\n\
    :type init_bias: scalar or NumPy array or :mod:`cntk.initializer`, defaults to\
    \ 0\n:param enable_self_stabilization: if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`\n\
    \                                  to all state-related projections (but not the\
    \ data input)\n:type enable_self_stabilization: bool, defaults to `False`\n:param\
    \ name: the name of the Function instance in the network\n:type name: str, defaults\
    \ to ''\n\n:returns: A function ``(prev_h, prev_c, input) -> (h, c)`` that implements\
    \ one step of a recurrent LSTM layer.\n:rtype: :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.layers.blocks.LSTM
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.RNNStep
  summary: "Layer factory function to create a plain RNN block for use inside a recurrence.\n\
    The RNN block implements one step of the recurrence and is stateless. It accepts\
    \ the previous state as its first argument,\nand outputs its new state.\n\n..\
    \ admonition:: Example\n\n   >>> # a plain relu RNN layer\n   >>> from cntk.layers\
    \ import *\n   >>> relu_rnn_layer = Recurrence(RNNStep(500, activation=C.relu))\n\
    \n:param shape: vector or tensor dimension of the output of this layer\n:type\
    \ shape: `int` or `tuple` of `ints`\n:param cell_shape: if given, then the output\
    \ state is first computed at `cell_shape`\n                   and linearly projected\
    \ to `shape`\n:type cell_shape: tuple, defaults to `None`\n:param activation:\
    \ function to apply at the end, e.g. `relu`\n:type activation: :class:`~cntk.ops.functions.Function`,\
    \ defaults to signmoid\n:param init: initial value of weights `W`\n:type init:\
    \ scalar or NumPy array or :mod:`cntk.initializer`, defaults to `glorot_uniform`\n\
    :param init_bias: initial value of weights `b`\n:type init_bias: scalar or NumPy\
    \ array or :mod:`cntk.initializer`, defaults to 0\n:param enable_self_stabilization:\
    \ if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`\n              \
    \                    to all state-related projections (but not the data input)\n\
    :type enable_self_stabilization: bool, defaults to `False`\n:param name: the name\
    \ of the Function instance in the network\n:type name: str, defaults to ''\n\n\
    :returns: A function ``(prev_h, input) -> h`` where ``h = activation(input @ W\
    \ + prev_h @ R + b)``\n:rtype: :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.layers.blocks.RNNStep
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.RNNUnit
  summary: 'This is a deprecated name for :func:`~cntk.layers.blocks.RNNStep`. Use
    that name instead.

    '
  type: Method
  uid: cntk.layers.blocks.RNNUnit
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.Stabilizer
  summary: "Layer factory function to create a `Droppo self-stabilizer <https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/SelfLR.pdf>`_.\n\
    It multiplies its input with a scalar that is learned.\n\nThis takes `enable_self_stabilization`\
    \ as a flag that allows to disable itself. Useful if this is a global default.\n\
    \n.. note::\n\n   Some other layers (specifically, recurrent units like :func:`~cntk.layers.blocks.LSTM`)\
    \ also have the option to\n   use the ``Stabilizer()`` layer internally. That\
    \ is enabled by passing `enable_self_stabilization=True`\n   to those layers.\
    \ In conjunction with those, the rule is that an explicit ``Stabilizer()`` must\
    \ be\n   inserted by the user for the main data input, whereas the recurrent layer\
    \ will own the stabilizer(s)\n   for the internal recurrent connection(s).\n\n\
    .. note::\n\n   Unlike the original paper, which proposed a linear or exponential\
    \ scalar,\n   CNTK uses a sharpened Softplus: 1/steepness ln(1+e^{steepness*beta}).\n\
    \   The softplus behaves linear for weights around and above 1 (like the linear\
    \ scalar) while guaranteeing\n   positiveness (like the exponentional variant)\
    \ but is also more robust by avoiding exploding gradients.\n\n.. admonition::\
    \ Example\n\n   >>> # recurrent model with self-stabilization\n   >>> from cntk.layers\
    \ import *\n   >>> with default_options(enable_self_stabilization=True): # enable\
    \ stabilizers by default for LSTM()\n   ...     model = Sequential([\n   ... \
    \        Embedding(300),\n   ...         Stabilizer(),           # stabilizer\
    \ for main data input of recurrence\n   ...         Recurrence(LSTM(512)),  #\
    \ LSTM owns its own stabilizers for the recurrent connections\n   ...        \
    \ Stabilizer(),\n   ...         Dense(10)\n   ...     ])\n\n:param steepness:\n\
    :type steepness: `int`, defaults to 4\n:param enable_self_stabilization: a flag\
    \ that allows to disable itself. Useful if this is a global default\n:type enable_self_stabilization:\
    \ bool, defaults to `False`\n:param name: the name of the Function instance in\
    \ the network\n:type name: str, defaults to ''\n\n:returns: A function\n:rtype:\
    \ :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.layers.blocks.Stabilizer
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.UntestedBranchError
  summary: ''
  type: Method
  uid: cntk.layers.blocks.UntestedBranchError
