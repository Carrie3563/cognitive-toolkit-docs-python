api_name: []
items:
- _type: module
  children:
  - cntk.learners.Learner
  - cntk.learners.UnitType
  - cntk.learners.UserLearner
  module: cntk.learners
  name: cntk.learners
  summary: 'Learner tunes a set of parameters during the training process. One can
    use

    different learners for different sets of parameters. Currently, CNTK supports

    the following learning algorithms:


    - AdaDelta

    - AdaGrad

    - FSAdaGrad

    - Adam

    - MomentumSGD

    - Nesterov

    - RMSProp

    - SGD

    '
  type: Namespace
  uid: cntk.learners
- _type: class
  children:
  - cntk.learners.adadelta
  - cntk.learners.adagrad
  - cntk.learners.adam
  - cntk.learners.default_unit_gain_value
  - cntk.learners.default_use_mean_gradient_value
  - cntk.learners.fsadagrad
  - cntk.learners.learning_rate_schedule
  - cntk.learners.momentum_as_time_constant_schedule
  - cntk.learners.momentum_schedule
  - cntk.learners.momentum_sgd
  - cntk.learners.nesterov
  - cntk.learners.rmsprop
  - cntk.learners.set_default_unit_gain_value
  - cntk.learners.set_default_use_mean_gradient_value
  - cntk.learners.sgd
  - cntk.learners.training_parameter_schedule
  - cntk.learners.universal
  module: cntk.learners
  name: cntk.learners.Global
  summary: Proxy object to hold module level functions
  type: Class
  uid: cntk.learners.Global
- _type: class
  children:
  - cntk.learners.Learner.learning_rate
  - cntk.learners.Learner.parameters
  - cntk.learners.Learner.reset_learning_rate
  - cntk.learners.Learner.update
  module: cntk.learners
  name: cntk.learners.Learner
  summary: 'Abstraction for learning a subset of parameters of a learnable function
    using first order gradient values.

    For example momentum, AdaGrad, RMSProp, etc. are different types of learners with
    their own algorithms for

    learning parameter values using first order gradients.

    To instantiate a concrete learner, use the factory methods in this module.

    '
  type: Class
  uid: cntk.learners.Learner
- _type: method
  class: cntk.learners.Learner
  module: cntk.learners
  name: cntk.learners.Learner.learning_rate
  summary: 'Current learning rate schedule.

    '
  type: Method
  uid: cntk.learners.Learner.learning_rate
- _type: attribute
  class: cntk.learners.Learner
  module: cntk.learners
  name: cntk.learners.Learner.parameters
  summary: The set of parameters associated with this learner.
  type: Property
  uid: cntk.learners.Learner.parameters
- _type: method
  class: cntk.learners.Learner
  module: cntk.learners
  name: cntk.learners.Learner.reset_learning_rate
  summary: 'Resets the learning rate. The new schedule is adjusted to be relative

    to the current number of elapsed samples/sweeps: the 0 offset in

    the new schedule corresponds to the current value of elapsed samples/sweeps,

    and it takes effect from the current position in the training process onwards.


    :param learning_rate: learning rate to reset to

    :type learning_rate: output of :func:`learning_rate_schedule`

    '
  type: Method
  uid: cntk.learners.Learner.reset_learning_rate
- _type: method
  class: cntk.learners.Learner
  module: cntk.learners
  name: cntk.learners.Learner.update
  summary: "Update the parameters associated with this learner.\n\n:param gradient_values:\
    \ maps :class:`~cntk.variables.Parameter` to\n                        a NumPy\
    \ array containing the first order gradient values for the\n                 \
    \       Parameter w.r.t. the training objective.\n:type gradient_values: dict\n\
    :param training_sample_count: number of samples in the minibatch\n:type training_sample_count:\
    \ int\n\n:returns: `False` to indicate that learning has stopped for all of the\
    \ parameters associated with this learner\n:rtype: bool\n"
  type: Method
  uid: cntk.learners.Learner.update
- _type: class
  children:
  - cntk.learners.UnitType.minibatch
  - cntk.learners.UnitType.sample
  module: cntk.learners
  name: cntk.learners.UnitType
  summary: 'Indicates whether the values in the schedule are specified on the per-sample
    or

    per-minibatch basis.

    '
  type: Class
  uid: cntk.learners.UnitType
- _type: attribute
  class: cntk.learners.UnitType
  module: cntk.learners
  name: cntk.learners.UnitType.minibatch
  summary: 'Schedule contains per-minibatch values (and need to be re-scaled by the
    learner

    using the actual minibatch size in samples).'
  type: Property
  uid: cntk.learners.UnitType.minibatch
- _type: attribute
  class: cntk.learners.UnitType
  module: cntk.learners
  name: cntk.learners.UnitType.sample
  summary: Schedule contains per-sample values.
  type: Property
  uid: cntk.learners.UnitType.sample
- _type: class
  children:
  - cntk.learners.UserLearner.update
  module: cntk.learners
  name: cntk.learners.UserLearner
  summary: 'Base class of all user-defined learners. To implement your own learning

    algorithm, derive from this class and override the :meth:`update`.


    Certain optimizers (such as AdaGrad) require additional storage.

    This can be allocated and initialized during construction.

    '
  type: Class
  uid: cntk.learners.UserLearner
- _type: method
  class: cntk.learners.UserLearner
  module: cntk.learners
  name: cntk.learners.UserLearner.update
  summary: "Update the parameters associated with this learner.\n\n:param gradient_values:\
    \ maps :class:`~cntk.variables.Parameter` to\n                        a NumPy\
    \ array containing the first order gradient values for the\n                 \
    \       Parameter w.r.t. the training objective.\n:type gradient_values: dict\n\
    :param training_sample_count: number of samples in the minibatch\n:type training_sample_count:\
    \ int\n:param sweep_end: if the data is fed by a conforming reader, this indicates\n\
    \                  whether a full pass over the dataset has just occurred.\n:type\
    \ sweep_end: bool\n\n:returns: `False` to indicate that learning has stopped for\
    \ all of the\n          parameters associated with this learner\n:rtype: bool\n"
  type: Method
  uid: cntk.learners.UserLearner.update
- _type: function
  module: cntk.learners
  name: cntk.learners.adadelta
  summary: "Creates an AdaDelta learner instance to learn the parameters. See [1]\
    \ for\nmore information.\n\n:param parameters: list of network parameters to tune.\n\
    \                   These can be obtained by the root operator's ``parameters``.\n\
    :type parameters: list of parameters\n:param lr: learning rate schedule.\n:type\
    \ lr: output of :func:`learning_rate_schedule`\n:param rho: exponential smooth\
    \ factor for each minibatch.\n:type rho: float\n:param epsilon: epsilon for sqrt.\n\
    :type epsilon: float\n:param l1_regularization_weight: the L1 regularization weight\
    \ per sample,\n                                 defaults to 0.0\n:type l1_regularization_weight:\
    \ float, optional\n:param l2_regularization_weight: the L2 regularization weight\
    \ per sample,\n                                 defaults to 0.0\n:type l2_regularization_weight:\
    \ float, optional\n:param gaussian_noise_injection_std_dev: the standard deviation\n\
    \                                         of the Gaussian noise added to parameters\
    \ post update, defaults to 0.0\n:type gaussian_noise_injection_std_dev: float,\
    \ optional\n:param gradient_clipping_threshold_per_sample: clipping threshold\n\
    \                                               per sample, defaults to infinity\n\
    :type gradient_clipping_threshold_per_sample: float, optional\n:param gradient_clipping_with_truncation:\
    \ use gradient clipping\n                                          with truncation\n\
    :type gradient_clipping_with_truncation: bool, default ``True``\n:param use_mean_gradient:\
    \ use averaged gradient as input to learner.\n                          Defaults\
    \ to the value returned by :func:`default_use_mean_gradient_value()`.\n:type use_mean_gradient:\
    \ bool, default ``False``\n\n:returns: learner instance that can be passed to\n\
    \          the :class:`~cntk.train.trainer.Trainer`\n:rtype: :class:`~cntk.learners.Learner`\n\
    \n.. seealso::\n\n   [1]  Matthew D. Zeiler, `ADADELTA: An Adaptive Learning Rate\
    \ Method\n   <https://arxiv.org/pdf/1212.5701.pdf>`_.\n"
  type: Method
  uid: cntk.learners.adadelta
- _type: function
  module: cntk.learners
  name: cntk.learners.adagrad
  summary: "Creates an AdaGrad learner instance to learn the parameters. See [1] for\n\
    more information.\n\n:param parameters: list of network parameters to tune.\n\
    \                   These can be obtained by the root operator's ``parameters``.\n\
    :type parameters: list of parameters\n:param lr: learning rate schedule.\n:type\
    \ lr: output of :func:`learning_rate_schedule`\n:param need_ave_multiplier:\n\
    :type need_ave_multiplier: bool, default\n:param l1_regularization_weight: the\
    \ L1 regularization weight per sample,\n                                 defaults\
    \ to 0.0\n:type l1_regularization_weight: float, optional\n:param l2_regularization_weight:\
    \ the L2 regularization weight per sample,\n                                 defaults\
    \ to 0.0\n:type l2_regularization_weight: float, optional\n:param gaussian_noise_injection_std_dev:\
    \ the standard deviation\n                                         of the Gaussian\
    \ noise added to parameters post update, defaults to 0.0\n:type gaussian_noise_injection_std_dev:\
    \ float, optional\n:param gradient_clipping_threshold_per_sample: clipping threshold\n\
    \                                               per sample, defaults to infinity\n\
    :type gradient_clipping_threshold_per_sample: float, optional\n:param gradient_clipping_with_truncation:\
    \ use gradient clipping\n                                          with truncation\n\
    :type gradient_clipping_with_truncation: bool, default ``True``\n:param use_mean_gradient:\
    \ use averaged gradient as input to learner.\n                          Defaults\
    \ to the value returned by :func:`default_use_mean_gradient_value()`.\n:type use_mean_gradient:\
    \ bool, default ``False``\n\n:returns: learner instance that can be passed to\n\
    \          the :class:`~cntk.train.trainer.Trainer`\n:rtype: :class:`~cntk.learners.Learner`\n\
    \n.. seealso::\n\n   [1]  J. Duchi, E. Hazan, and Y. Singer. `Adaptive Subgradient\
    \ Methods\n   for Online Learning and Stochastic Optimization\n   <http://www.magicbroom.info/Papers/DuchiHaSi10.pdf>`_.\
    \ The Journal of\n   Machine Learning Research, 2011.\n"
  type: Method
  uid: cntk.learners.adagrad
- _type: function
  module: cntk.learners
  name: cntk.learners.adam
  summary: "Creates an Adam learner instance to learn the parameters. See [1] for\
    \ more\ninformation.\n\n:param parameters: list of network parameters to tune.\n\
    \                   These can be obtained by the root operator's ``parameters``.\n\
    :type parameters: list of parameters\n:param lr: learning rate schedule.\n:type\
    \ lr: output of :func:`learning_rate_schedule`\n:param momentum: momentum schedule.\n\
    \                 For additional information, please refer to the :cntkwiki:`this\
    \ CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`.\n\
    :type momentum: output of :func:`momentum_schedule` or :func:`momentum_as_time_constant_schedule`\n\
    :param unit_gain: when ``True``, momentum is interpreted as a unit-gain filter.\
    \ Defaults\n                  to the value returned by :func:`default_unit_gain_value`.\n\
    :param variance_momentum: variance momentum schedule. Defaults\n             \
    \             to ``momentum_as_time_constant_schedule(720000)``.\n:type variance_momentum:\
    \ output of :func:`momentum_schedule` or :func:`momentum_as_time_constant_schedule`\n\
    :param l1_regularization_weight: the L1 regularization weight per sample,\n  \
    \                               defaults to 0.0\n:type l1_regularization_weight:\
    \ float, optional\n:param l2_regularization_weight: the L2 regularization weight\
    \ per sample,\n                                 defaults to 0.0\n:type l2_regularization_weight:\
    \ float, optional\n:param gaussian_noise_injection_std_dev: the standard deviation\n\
    \                                         of the Gaussian noise added to parameters\
    \ post update, defaults to 0.0\n:type gaussian_noise_injection_std_dev: float,\
    \ optional\n:param gradient_clipping_threshold_per_sample: clipping threshold\n\
    \                                               per sample, defaults to infinity\n\
    :type gradient_clipping_threshold_per_sample: float, optional\n:param gradient_clipping_with_truncation:\
    \ use gradient clipping\n                                          with truncation\n\
    :type gradient_clipping_with_truncation: bool, default ``True``\n:param use_mean_gradient:\
    \ use averaged gradient as input to learner.\n                          Defaults\
    \ to the value returned by :func:`default_use_mean_gradient_value()`.\n:type use_mean_gradient:\
    \ bool, default ``False``\n:param epsilon: numerical stability constant,\n   \
    \             defaults to 1e-8\n:type epsilon: float, optional\n:param adamax:\
    \ when ``True``, use infinity-norm variance momentum update instead of L2. Defaults\n\
    \               to False\n\n:returns: learner instance that can be passed to\n\
    \          the :class:`~cntk.train.trainer.Trainer`\n:rtype: :class:`~cntk.learners.Learner`\n\
    \n.. seealso::\n\n   [1] D. Kingma, J. Ba. `Adam: A Method for Stochastic Optimization\n\
    \   <https://arxiv.org/abs/1412.6980>`_. International Conference for\n   Learning\
    \ Representations, 2015.\n"
  type: Method
  uid: cntk.learners.adam
- _type: function
  module: cntk.learners
  name: cntk.learners.default_unit_gain_value
  summary: 'Returns true if by default momentum is applied in the unit-gain fashion.

    '
  type: Method
  uid: cntk.learners.default_unit_gain_value
- _type: function
  module: cntk.learners
  name: cntk.learners.default_use_mean_gradient_value
  summary: 'Returns true if by default input gradient to learner is averaged.

    '
  type: Method
  uid: cntk.learners.default_use_mean_gradient_value
- _type: function
  module: cntk.learners
  name: cntk.learners.fsadagrad
  summary: "Creates an FSAdaGrad learner instance to learn the parameters.\n\n:param\
    \ parameters: list of network parameters to tune.\n                   These can\
    \ be obtained by the root operator's ``parameters``.\n:type parameters: list of\
    \ parameters\n:param lr: learning rate schedule.\n:type lr: output of :func:`learning_rate_schedule`\n\
    :param momentum: momentum schedule.\n                 For additional information,\
    \ please refer to the :cntkwiki:`this CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`.\n\
    :type momentum: output of :func:`momentum_schedule` or :func:`momentum_as_time_constant_schedule`\n\
    :param unit_gain: when ``True``, momentum is interpreted as a unit-gain filter.\
    \ Defaults\n                  to the value returned by :func:`default_unit_gain_value`.\n\
    :param variance_momentum: variance momentum schedule. Defaults\n             \
    \             to ``momentum_as_time_constant_schedule(720000)``.\n:type variance_momentum:\
    \ output of :func:`momentum_schedule` or :func:`momentum_as_time_constant_schedule`\n\
    :param l1_regularization_weight: the L1 regularization weight per sample,\n  \
    \                               defaults to 0.0\n:type l1_regularization_weight:\
    \ float, optional\n:param l2_regularization_weight: the L2 regularization weight\
    \ per sample,\n                                 defaults to 0.0\n:type l2_regularization_weight:\
    \ float, optional\n:param gaussian_noise_injection_std_dev: the standard deviation\n\
    \                                         of the Gaussian noise added to parameters\
    \ post update, defaults to 0.0\n:type gaussian_noise_injection_std_dev: float,\
    \ optional\n:param gradient_clipping_threshold_per_sample: clipping threshold\n\
    \                                               per sample, defaults to infinity\n\
    :type gradient_clipping_threshold_per_sample: float, optional\n:param gradient_clipping_with_truncation:\
    \ use gradient clipping\n                                          with truncation\n\
    :type gradient_clipping_with_truncation: bool, default ``True``\n:param use_mean_gradient:\
    \ use averaged gradient as input to learner.\n                          Defaults\
    \ to the value returned by :func:`default_use_mean_gradient_value()`.\n:type use_mean_gradient:\
    \ bool, default ``False``\n\n:returns: learner instance that can be passed to\n\
    \          the :class:`~cntk.train.trainer.Trainer`\n:rtype: :class:`~cntk.learners.Learner`\n"
  type: Method
  uid: cntk.learners.fsadagrad
- _type: function
  module: cntk.learners
  name: cntk.learners.learning_rate_schedule
  summary: "Create a learning rate schedule (using the same semantics as\n:func:`training_parameter_schedule`).\n\
    \n:param lr: see parameter ``schedule`` in\n           :func:`training_parameter_schedule`.\n\
    :type lr: float or list\n:param unit: see parameter\n             ``unit`` in\
    \ :func:`training_parameter_schedule`.\n:type unit: :class:`UnitType`\n:param\
    \ epoch_size: see parameter ``epoch_size`` in\n                   :func:`training_parameter_schedule`.\n\
    :type epoch_size: int\n\n:returns: learning rate schedule\n\n.. seealso:: :func:`training_parameter_schedule`\n"
  type: Method
  uid: cntk.learners.learning_rate_schedule
- _type: function
  module: cntk.learners
  name: cntk.learners.momentum_as_time_constant_schedule
  summary: "Create a momentum schedule in a minibatch-size agnostic way\n(using the\
    \ same semantics as :func:`training_parameter_schedule`\nwith `unit=UnitType.sample`).\n\
    \n:param momentum: see parameter ``schedule`` in\n                 :func:`training_parameter_schedule`.\n\
    :type momentum: float or list\n:param epoch_size: see parameter ``epoch_size``\
    \ in\n                   :func:`training_parameter_schedule`.\n:type epoch_size:\
    \ int\n\nCNTK specifies momentum in a minibatch-size agnostic way as the time\n\
    constant (in samples) of a unit-gain 1st-order IIR filter. The value\nspecifies\
    \ the number of samples after which a gradient has an effect of\n1/e=37%.\n\n\
    If you want to specify the momentum per sample (or per minibatch),\nuse :func:`momentum_schedule`.\n\
    \n.. admonition:: Examples\n\n   >>> # Use a fixed momentum of 1100 for all samples\n\
    \   >>> m = momentum_as_time_constant_schedule(1100)\n   \n   >>> # Use the time\
    \ constant 1100 for the first 1000 samples,\n   >>> # then 1500 for the remaining\
    \ ones\n   >>> m = momentum_as_time_constant_schedule([1100, 1500], 1000)\n\n\
    :returns: momentum as time constant schedule\n"
  type: Method
  uid: cntk.learners.momentum_as_time_constant_schedule
- _type: function
  module: cntk.learners
  name: cntk.learners.momentum_schedule
  summary: "Create a per-minibatch momentum schedule (using the same semantics as\n\
    :func:`training_parameter_schedule` with the `unit=UnitType.minibatch`).\n\n:param\
    \ momentum: see parameter ``schedule`` in\n                 :func:`training_parameter_schedule`.\n\
    :type momentum: float or list\n:param epoch_size: see parameter ``epoch_size``\
    \ in\n                   :func:`training_parameter_schedule`.\n:type epoch_size:\
    \ int\n\nIf you want to provide momentum values in a minibatch-size\nagnostic\
    \ way, use :func:`momentum_as_time_constant_schedule`.\n\n.. admonition:: Examples\n\
    \n   >>> # Use a fixed momentum of 0.99 for all samples\n   >>> m = momentum_schedule(0.99)\n\
    \   \n   >>> # Use the momentum value 0.99 for the first 1000 samples,\n   >>>\
    \ # then 0.9 for the remaining ones\n   >>> m = momentum_schedule([0.99,0.9],\
    \ 1000)\n   >>> m[0], m[999], m[1000], m[1001]\n   (0.99, 0.99, 0.9, 0.9)\n  \
    \ \n   >>> # Use the momentum value 0.99 for the first 999 samples,\n   >>> #\
    \ then 0.88 for the next 888 samples, and 0.77 for the\n   >>> # the remaining\
    \ ones\n   >>> m = momentum_schedule([(999,0.99),(888,0.88),(0, 0.77)])\n   >>>\
    \ m[0], m[998], m[999], m[999+888-1], m[999+888]\n   (0.99, 0.99, 0.88, 0.88,\
    \ 0.77)\n\n:returns: momentum schedule\n"
  type: Method
  uid: cntk.learners.momentum_schedule
- _type: function
  module: cntk.learners
  name: cntk.learners.momentum_sgd
  summary: "Creates a Momentum SGD learner instance to learn the parameters.\n\n:param\
    \ parameters: list of network parameters to tune.\n                   These can\
    \ be obtained by the root operator's ``parameters``.\n:type parameters: list of\
    \ parameters\n:param lr: learning rate schedule.\n:type lr: output of :func:`learning_rate_schedule`\n\
    :param momentum: momentum schedule.\n                 For additional information,\
    \ please refer to the :cntkwiki:`this CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`.\n\
    :type momentum: output of :func:`momentum_schedule` or :func:`momentum_as_time_constant_schedule`\n\
    :param unit_gain: when ``True``, momentum is interpreted as a unit-gain filter.\
    \ Defaults\n                  to the value returned by :func:`default_unit_gain_value`.\n\
    :param l1_regularization_weight: the L1 regularization weight per sample,\n  \
    \                               defaults to 0.0\n:type l1_regularization_weight:\
    \ float, optional\n:param l2_regularization_weight: the L2 regularization weight\
    \ per sample,\n                                 defaults to 0.0\n:type l2_regularization_weight:\
    \ float, optional\n:param gaussian_noise_injection_std_dev: the standard deviation\n\
    \                                         of the Gaussian noise added to parameters\
    \ post update, defaults to 0.0\n:type gaussian_noise_injection_std_dev: float,\
    \ optional\n:param gradient_clipping_threshold_per_sample: clipping threshold\n\
    \                                               per sample, defaults to infinity\n\
    :type gradient_clipping_threshold_per_sample: float, optional\n:param gradient_clipping_with_truncation:\
    \ use gradient clipping\n                                          with truncation\n\
    :type gradient_clipping_with_truncation: bool, default ``True``\n:param use_mean_gradient:\
    \ use averaged gradient as input to learner.\n                          Defaults\
    \ to the value returned by :func:`default_use_mean_gradient_value()`.\n:type use_mean_gradient:\
    \ bool, default ``False``\n\n:returns: learner instance that can be passed to\n\
    \          the :class:`~cntk.train.trainer.Trainer`\n:rtype: :class:`~cntk.learners.Learner`\n"
  type: Method
  uid: cntk.learners.momentum_sgd
- _type: function
  module: cntk.learners
  name: cntk.learners.nesterov
  summary: "Creates a Nesterov SGD learner instance to learn the parameters. This\
    \ was\noriginally proposed by Nesterov [1] in 1983 and then shown to work well\
    \ in\na deep learning context by Sutskever, et al. [2].\n\n:param parameters:\
    \ list of network parameters to tune.\n                   These can be obtained\
    \ by the root operator's ``parameters``.\n:type parameters: list of parameters\n\
    :param lr: learning rate schedule.\n:type lr: output of :func:`learning_rate_schedule`\n\
    :param momentum: momentum schedule.\n                 For additional information,\
    \ please refer to the :cntkwiki:`this CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`.\n\
    :type momentum: output of :func:`momentum_schedule` or :func:`momentum_as_time_constant_schedule`\n\
    :param unit_gain: when ``True``, momentum is interpreted as a unit-gain filter.\
    \ Defaults\n                  to the value returned by :func:`default_unit_gain_value`.\n\
    :param l1_regularization_weight: the L1 regularization weight per sample,\n  \
    \                               defaults to 0.0\n:type l1_regularization_weight:\
    \ float, optional\n:param l2_regularization_weight: the L2 regularization weight\
    \ per sample,\n                                 defaults to 0.0\n:type l2_regularization_weight:\
    \ float, optional\n:param gaussian_noise_injection_std_dev: the standard deviation\n\
    \                                         of the Gaussian noise added to parameters\
    \ post update, defaults to 0.0\n:type gaussian_noise_injection_std_dev: float,\
    \ optional\n:param gradient_clipping_threshold_per_sample: clipping threshold\n\
    \                                               per sample, defaults to infinity\n\
    :type gradient_clipping_threshold_per_sample: float, optional\n:param gradient_clipping_with_truncation:\
    \ use gradient clipping\n                                          with truncation\n\
    :type gradient_clipping_with_truncation: bool, default ``True``\n:param use_mean_gradient:\
    \ use averaged gradient as input to learner.\n                          Defaults\
    \ to the value returned by :func:`default_use_mean_gradient_value()`.\n:type use_mean_gradient:\
    \ bool, default ``False``\n\n:returns: learner instance that can be passed to\n\
    \          the :class:`~cntk.train.trainer.Trainer`\n:rtype: :class:`~cntk.learners.Learner`\n\
    \n.. seealso::\n\n   [1] Y. Nesterov. A Method of Solving a Convex Programming\
    \ Problem with Convergence Rate O(1/ sqrt(k)). Soviet Mathematics Doklady, 1983.\n\
    \   \n   [2] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. `On the\n   Importance\
    \ of Initialization and Momentum in Deep Learning\n   <http://www.cs.toronto.edu/~fritz/absps/momentum.pdf>`_.\
    \  Proceedings\n   of the 30th International Conference on Machine Learning, 2013.\n"
  type: Method
  uid: cntk.learners.nesterov
- _type: function
  module: cntk.learners
  name: cntk.learners.rmsprop
  summary: "Creates an RMSProp learner instance to learn the parameters.\n\n:param\
    \ parameters: list of network parameters to tune.\n                   These can\
    \ be obtained by the root operator's ``parameters``.\n:type parameters: list of\
    \ parameters\n:param lr: learning rate schedule.\n:type lr: output of :func:`learning_rate_schedule`\n\
    :param gamma: Trade-off factor for current and previous gradients. Common value\
    \ is 0.95. Should be in range (0.0, 1.0)\n:type gamma: float\n:param inc: Increasing\
    \ factor when trying to adjust current learning_rate. Should be greater than 1\n\
    :type inc: float\n:param dec: Decreasing factor when trying to adjust current\
    \ learning_rate. Should be in range (0.0, 1.0)\n:type dec: float\n:param max:\
    \ Maximum scale allowed for the initial learning_rate. Should be greater than\
    \ zero and min\n:type max: float\n:param min: Minimum scale allowed for the initial\
    \ learning_rate. Should be greater than zero\n:type min: float\n:param need_ave_multiplier:\n\
    :type need_ave_multiplier: bool, default ``True``\n:param l1_regularization_weight:\
    \ the L1 regularization weight per sample,\n                                 defaults\
    \ to 0.0\n:type l1_regularization_weight: float, optional\n:param l2_regularization_weight:\
    \ the L2 regularization weight per sample,\n                                 defaults\
    \ to 0.0\n:type l2_regularization_weight: float, optional\n:param gaussian_noise_injection_std_dev:\
    \ the standard deviation\n                                         of the Gaussian\
    \ noise added to parameters post update, defaults to 0.0\n:type gaussian_noise_injection_std_dev:\
    \ float, optional\n:param gradient_clipping_threshold_per_sample: clipping threshold\n\
    \                                               per sample, defaults to infinity\n\
    :type gradient_clipping_threshold_per_sample: float, optional\n:param gradient_clipping_with_truncation:\
    \ use gradient clipping\n                                          with truncation\n\
    :type gradient_clipping_with_truncation: bool, default ``True``\n:param use_mean_gradient:\
    \ use averaged gradient as input to learner.\n                          Defaults\
    \ to the value returned by :func:`default_use_mean_gradient_value()`.\n:type use_mean_gradient:\
    \ bool, default ``False``\n\n:returns: learner instance that can be passed to\n\
    \          the :class:`~cntk.train.trainer.Trainer`\n:rtype: :class:`~cntk.learners.Learner`\n"
  type: Method
  uid: cntk.learners.rmsprop
- _type: function
  module: cntk.learners
  name: cntk.learners.set_default_unit_gain_value
  summary: 'Sets globally default unit-gain flag value.

    '
  type: Method
  uid: cntk.learners.set_default_unit_gain_value
- _type: function
  module: cntk.learners
  name: cntk.learners.set_default_use_mean_gradient_value
  summary: 'Sets globally default use_mean_gradient_value.

    '
  type: Method
  uid: cntk.learners.set_default_use_mean_gradient_value
- _type: function
  module: cntk.learners
  name: cntk.learners.sgd
  summary: "Creates an SGD learner instance to learn the parameters. See [1] for more\n\
    information on how to set the parameters.\n\n:param parameters: list of network\
    \ parameters to tune.\n                   These can be obtained by the '.parameters()'\
    \ method of the root\n                   operator.\n:type parameters: list of\
    \ parameters\n:param lr: learning rate schedule.\n:type lr: output of :func:`learning_rate_schedule`\n\
    :param l1_regularization_weight: the L1 regularization weight per sample,\n  \
    \                               defaults to 0.0\n:type l1_regularization_weight:\
    \ float, optional\n:param l2_regularization_weight: the L2 regularization weight\
    \ per sample,\n                                 defaults to 0.0\n:type l2_regularization_weight:\
    \ float, optional\n:param gaussian_noise_injection_std_dev: the standard deviation\n\
    \                                         of the Gaussian noise added to parameters\
    \ post update, defaults to 0.0\n:type gaussian_noise_injection_std_dev: float,\
    \ optional\n:param gradient_clipping_threshold_per_sample: clipping threshold\n\
    \                                               per sample, defaults to infinity\n\
    :type gradient_clipping_threshold_per_sample: float, optional\n:param gradient_clipping_with_truncation:\
    \ use gradient clipping\n                                          with truncation\n\
    :type gradient_clipping_with_truncation: bool, default ``True``\n:param use_mean_gradient:\
    \ use averaged gradient as input to learner.\n                          Defaults\
    \ to the value returned by :func:`default_use_mean_gradient_value()`.\n:type use_mean_gradient:\
    \ bool, default ``False``\n\n:returns: learner instance that can be passed to\n\
    \          the :class:`~cntk.train.trainer.Trainer`\n:rtype: :class:`~cntk.learners.Learner`\n\
    \n.. seealso::\n\n   [1] L. Bottou. `Stochastic Gradient Descent Tricks\n   <https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks>`_.\
    \ Neural\n   Networks: Tricks of the Trade: Springer, 2012.\n"
  type: Method
  uid: cntk.learners.sgd
- _type: function
  module: cntk.learners
  name: cntk.learners.training_parameter_schedule
  summary: "Create a training parameter schedule containing either per-sample (default)\n\
    or per-minibatch values.\n\n.. admonition:: Examples\n\n   >>> # Use a fixed value\
    \ 0.01 for all samples\n   >>> s = training_parameter_schedule(0.01, UnitType.sample)\n\
    \   >>> s[0], s[1]\n   (0.01, 0.01)\n   \n   >>> # Use 0.01 for the first 1000\
    \ samples, then 0.001 for the remaining ones\n   >>> s = training_parameter_schedule([0.01,\
    \ 0.001], UnitType.sample, 1000)\n   >>> s[0], s[1], s[1000], s[1001]\n   (0.01,\
    \ 0.01, 0.001, 0.001)\n   \n   >>> # Use 0.1 for the first 12 epochs, then 0.01\
    \ for the next 15,\n   >>> # followed by 0.001 for the remaining ones, with a\
    \ 100 samples in an epoch\n   >>> s = training_parameter_schedule([(12, 0.1),\
    \ (15, 0.01), (1, 0.001)], UnitType.sample, 100)\n   >>> s[0], s[1199], s[1200],\
    \ s[2699], s[2700], s[5000]\n   (0.1, 0.1, 0.01, 0.01, 0.001, 0.001)\n\n:param\
    \ schedule: if float, is the parameter schedule to be used\n                 for\
    \ all samples. In case of list, the elements are used as the\n               \
    \  values for ``epoch_size`` samples. If list contains pair, the second element\
    \ is\n                 used as a value for (``epoch_size`` x first element) samples\n\
    :type schedule: float or list\n:param unit: one of two\n             * ``sample``:\
    \ the returned schedule contains per-sample values\n             * ``minibatch``:\
    \ the returned schedule contains per-minibatch values.\n:type unit: :class:`UnitType`\n\
    :param epoch_size: number of samples as a scheduling unit.\n                 \
    \  Parameters in the schedule change their values every ``epoch_size``\n     \
    \              samples. If no ``epoch_size`` is provided, this parameter is substituted\n\
    \                   by the size of the full data sweep, in which case the scheduling\
    \ unit is\n                   the entire data sweep (as indicated by the MinibatchSource)\
    \ and parameters\n                   change their values on the sweep-by-sweep\
    \ basis specified by the\n                   ``schedule``.\n:type epoch_size:\
    \ optional, int\n\n:returns: training parameter schedule\n\n.. seealso:: :func:`learning_rate_schedule`\n"
  type: Method
  uid: cntk.learners.training_parameter_schedule
- _type: function
  module: cntk.learners
  name: cntk.learners.universal
  summary: "Creates a learner which uses a CNTK function to update the parameters.\n\
    \n:param update_func: function that takes parameters and gradients as arguments\
    \ and\n                    returns a :class:`~cntk.ops.functions.Function` that\
    \ performs the\n                    desired updates. The returned function updates\
    \ the parameters by\n                    means of containing :func:`~cntk.ops.assign`\
    \ operations.\n                    If ``update_func`` does not contain :func:`~cntk.ops.assign`\
    \ operations\n                    the parameters will not be updated.\n:param\
    \ parameters: list of network parameters to tune.\n                   These can\
    \ be obtained by the root operator's `parameters`.\n:type parameters: list\n\n\
    :returns: learner instance that can be passed to\n          the :class:`~cntk.train.trainer.Trainer`\n\
    :rtype: :class:`~cntk.learners.Learner`\n\n.. admonition:: Examples\n\n   >>>\
    \ def my_adagrad(parameters, gradients):\n   ...     accumulators = [C.constant(0,\
    \ shape=p.shape, dtype=p.dtype, name='accum') for p in parameters]\n   ...   \
    \  update_funcs = []\n   ...     for p, g, a in zip(parameters, gradients, accumulators):\n\
    \   ...         accum_new = C.assign(a, g * g)\n   ...         update_funcs.append(C.assign(p,\
    \ p - 0.01 * g / C.sqrt(accum_new + 1e-6)))\n   ...     return C.combine(update_funcs)\n\
    \   ...\n   >>> x = C.input_variable((10,))\n   >>> y = C.input_variable((2,))\n\
    \   >>> z = C.layers.Sequential([C.layers.Dense(100, activation=C.relu), C.layers.Dense(2)])(x)\n\
    \   >>> loss = C.cross_entropy_with_softmax(z, y)\n   >>> learner = C.universal(my_adagrad,\
    \ z.parameters)\n   >>> trainer = C.Trainer(z, loss, learner)\n   >>> # now trainer\
    \ can be used as any other Trainer\n"
  type: Method
  uid: cntk.learners.universal
