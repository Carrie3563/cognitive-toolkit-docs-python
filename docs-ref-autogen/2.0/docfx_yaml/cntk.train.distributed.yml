api_name: []
items:
- _type: module
  children:
  - cntk.train.distributed.Communicator
  - cntk.train.distributed.DistributedLearner
  - cntk.train.distributed.WorkerDescriptor
  module: cntk.train.distributed
  name: cntk.train.distributed
  summary: 'Distributed learners manage learners in distributed environment.

    '
  type: Namespace
  uid: cntk.train.distributed
- _type: class
  children:
  - cntk.train.distributed.block_momentum_distributed_learner
  - cntk.train.distributed.data_parallel_distributed_learner
  - cntk.train.distributed.mpi_communicator
  module: cntk.train.distributed
  name: cntk.train.distributed.Global
  summary: Proxy object to hold module level functions
  type: Class
  uid: cntk.train.distributed.Global
- _type: class
  children:
  - cntk.train.distributed.Communicator.barrier
  - cntk.train.distributed.Communicator.current_worker
  - cntk.train.distributed.Communicator.finalize
  - cntk.train.distributed.Communicator.is_main
  - cntk.train.distributed.Communicator.num_workers
  - cntk.train.distributed.Communicator.rank
  - cntk.train.distributed.Communicator.workers
  module: cntk.train.distributed
  name: cntk.train.distributed.Communicator
  summary: 'A communicator interface exposing communication primitives that serve
    as building blocks

    for distributed training.

    '
  type: Class
  uid: cntk.train.distributed.Communicator
- _type: method
  class: cntk.train.distributed.Communicator
  module: cntk.train.distributed
  name: cntk.train.distributed.Communicator.barrier
  summary: 'Sync point to make sure all workers reach the same state.

    '
  type: Method
  uid: cntk.train.distributed.Communicator.barrier
- _type: method
  class: cntk.train.distributed.Communicator
  module: cntk.train.distributed
  name: cntk.train.distributed.Communicator.current_worker
  summary: 'Returns worker descriptor of current process.


    :returns: descriptor of current process.

    :rtype: :class:`WorkerDescriptor`

    '
  type: Method
  uid: cntk.train.distributed.Communicator.current_worker
- _type: method
  class: cntk.train.distributed.Communicator
  module: cntk.train.distributed
  name: cntk.train.distributed.Communicator.finalize
  summary: 'Should be called when all communication is finished. No more communication
    should happen after this call.

    '
  type: Method
  uid: cntk.train.distributed.Communicator.finalize
- _type: method
  class: cntk.train.distributed.Communicator
  module: cntk.train.distributed
  name: cntk.train.distributed.Communicator.is_main
  summary: 'Indicates if the current communicator is instantiated on the main node.
    The node with rank 0 is considered the main.

    '
  type: Method
  uid: cntk.train.distributed.Communicator.is_main
- _type: method
  class: cntk.train.distributed.Communicator
  module: cntk.train.distributed
  name: cntk.train.distributed.Communicator.num_workers
  summary: 'Returns information about all MPI workers.

    '
  type: Method
  uid: cntk.train.distributed.Communicator.num_workers
- _type: method
  class: cntk.train.distributed.Communicator
  module: cntk.train.distributed
  name: cntk.train.distributed.Communicator.rank
  summary: 'Returns rank of current process.

    '
  type: Method
  uid: cntk.train.distributed.Communicator.rank
- _type: method
  class: cntk.train.distributed.Communicator
  module: cntk.train.distributed
  name: cntk.train.distributed.Communicator.workers
  summary: 'Returns workers in this communicator.


    :returns: workers in this communicator.

    :rtype: (`list`) of :class:`WorkerDescriptor`

    '
  type: Method
  uid: cntk.train.distributed.Communicator.workers
- _type: class
  children:
  - cntk.train.distributed.DistributedLearner.communicator
  - cntk.train.distributed.DistributedLearner.total_number_of_samples_seen
  module: cntk.train.distributed
  name: cntk.train.distributed.DistributedLearner
  summary: 'A distributed learner that handles data like gradients/momentums across
    multiple MPI workers

    '
  type: Class
  uid: cntk.train.distributed.DistributedLearner
- _type: method
  class: cntk.train.distributed.DistributedLearner
  module: cntk.train.distributed
  name: cntk.train.distributed.DistributedLearner.communicator
  summary: 'Returns the distributed communicator that talks to other MPI workers


    :returns: descriptor of current process.

    :rtype: :class:`Communicator`

    '
  type: Method
  uid: cntk.train.distributed.DistributedLearner.communicator
- _type: attribute
  class: cntk.train.distributed.DistributedLearner
  module: cntk.train.distributed
  name: cntk.train.distributed.DistributedLearner.total_number_of_samples_seen
  summary: The number of samples seen by the distributed learner.
  type: Property
  uid: cntk.train.distributed.DistributedLearner.total_number_of_samples_seen
- _type: class
  children:
  - cntk.train.distributed.WorkerDescriptor.global_rank
  - cntk.train.distributed.WorkerDescriptor.host_id
  module: cntk.train.distributed
  name: cntk.train.distributed.WorkerDescriptor
  summary: 'Distributed worker descriptor, returned by :class:`Communicator` instance.

    '
  type: Class
  uid: cntk.train.distributed.WorkerDescriptor
- _type: attribute
  class: cntk.train.distributed.WorkerDescriptor
  module: cntk.train.distributed
  name: cntk.train.distributed.WorkerDescriptor.global_rank
  summary: The global rank of the worker.
  type: Property
  uid: cntk.train.distributed.WorkerDescriptor.global_rank
- _type: attribute
  class: cntk.train.distributed.WorkerDescriptor
  module: cntk.train.distributed
  name: cntk.train.distributed.WorkerDescriptor.host_id
  summary: The host id of the worker.
  type: Property
  uid: cntk.train.distributed.WorkerDescriptor.host_id
- _type: function
  module: cntk.train.distributed
  name: cntk.train.distributed.block_momentum_distributed_learner
  summary: "Creates a block momentum distributed learner. See [1] for more\ninformation.\n\
    \nBlock Momentum divides the full dataset into M non-overlapping blocks,\nand\
    \ each block is partitioned into N non-overlapping splits.\n\nDuring training,\
    \ a random, unprocessed block is randomly taken by the trainer\nand the N partitions\
    \ of this block are dispatched on the workers.\n\n:param learner: a local learner\
    \ (i.e. sgd)\n:param block_size: size of the partition in samples\n:type block_size:\
    \ int\n:param block_momentum_as_time_constant: block momentum as time constant\n\
    :type block_momentum_as_time_constant: float\n:param use_nestrov_momentum: use\
    \ nestrov momentum\n:type use_nestrov_momentum: bool\n:param reset_sgd_momentum_after_aggregation:\
    \ reset SGD momentum after aggregation\n:type reset_sgd_momentum_after_aggregation:\
    \ bool\n:param block_learning_rate: block learning rate\n:type block_learning_rate:\
    \ float\n:param distributed_after: number of samples after which distributed training\
    \ starts\n:type distributed_after: int\n\n:returns: a distributed learner instance\n\
    \n.. seealso::\n\n   [1] K. Chen and Q. Huo. `Scalable training of deep learning\
    \ machines\n   by incremental block training with intra-block parallel optimization\n\
    \   and blockwise model-update filtering\n   <https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/0005880.pdf>`_.\n\
    \   Proceedings of ICASSP, 2016.\n"
  type: Method
  uid: cntk.train.distributed.block_momentum_distributed_learner
- _type: function
  module: cntk.train.distributed
  name: cntk.train.distributed.data_parallel_distributed_learner
  summary: 'Creates a data parallel distributed learner


    :param learner: a local learner (i.e. sgd)

    :param distributed_after: number of samples after which distributed training starts

    :type distributed_after: int

    :param num_quantization_bits: number of bits for quantization (1 to 32)

    :type num_quantization_bits: int

    :param use_async_buffered_parameter_update: use async buffered parameter update

    :type use_async_buffered_parameter_update: bool


    :returns: a distributed learner instance

    '
  type: Method
  uid: cntk.train.distributed.data_parallel_distributed_learner
- _type: function
  module: cntk.train.distributed
  name: cntk.train.distributed.mpi_communicator
  summary: 'Creates a non quantized MPI communicator.

    '
  type: Method
  uid: cntk.train.distributed.mpi_communicator
